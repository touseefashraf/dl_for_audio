{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self,num_in=3,num_hid=[3,5],num_out=2):\n",
    "        self.num_input=num_in\n",
    "        self.num_hidden=num_hid\n",
    "        self.num_outputs=num_out\n",
    "        \n",
    "    # this list represent the number of neuronrs in a layer \n",
    "        layers=[self.num_input]+self.num_hidden+[self.num_outputs]\n",
    "\n",
    "\n",
    "        # random weights \n",
    "        self.weights=[]\n",
    "        for i in range(len(layers)-1): #as weights are in between two layers so we used layers len-1. eg 3 layers require 2 weights matrix\n",
    "            # we want a matrix w of weights with diminsion number of incoming neuron rows and next layer no. of neuron colums \n",
    "            w=np.random.rand(layers[i],layers[i+1])\n",
    "            self.weights.append(w)\n",
    "        \n",
    "        # initializing activations for storage\n",
    "        activations=[]\n",
    "        for i in range(len(layers)):\n",
    "            a=np.zeros(layers[i])\n",
    "            activations.append(a)\n",
    "        self.activations=activations\n",
    "        \n",
    "        # initializing derivatives for storage\n",
    "        derivatives=[]\n",
    "        for i in range(len(layers)):\n",
    "            d=np.zeros(layers[i],layers[i+1])\n",
    "            derivatives.append(d)\n",
    "        self.derivatives=derivatives\n",
    "            \n",
    "    def forward_propagation(self,inputs):\n",
    "        activation=inputs\n",
    "        self.activations[0]=inputs\n",
    "        for w in self.weights:\n",
    "            net=np.dot(activation,w)\n",
    "            \n",
    "            activation=self.sigmoid(net)\n",
    "        return activation\n",
    "    \n",
    "    def sigmoid(self,inp):\n",
    "        return 1/(1+np.exp(-inp)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input was [0.46267878 0.08158283 0.44795412].\n",
      "The output is [0.73417654 0.67289816]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    mlp=MLP()\n",
    "    \n",
    "    inputs=np.random.rand(mlp.num_input)\n",
    "    \n",
    "    outputs=mlp.forward_propagation(inputs)\n",
    "    \n",
    "    print('The input was {}.'.format(inputs))\n",
    "    \n",
    "    print(f'The output is {outputs}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "872297596d866abac5f8ee2e50e2eeb543af31a9f9c1bd6008203e0c51f332d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
